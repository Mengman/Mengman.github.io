I"Ü<p><strong>è®ºæ–‡é¢˜ç›®:</strong> On large-batch training for deep leanring: generalization gap and sharp minima</p>

<p><strong>è®ºæ–‡å†…å®¹ï¼š</strong> åœ¨SGDä¸­ä½¿ç”¨è¾ƒå¤§çš„batch-sizeä¼šå¯¼è‡´æ¨¡å‹çš„æ³›åŒ–èƒ½åŠ›é€€åŒ–ï¼Œå› ä¸ºè¾ƒå¤§çš„batch size æ¨¡å‹æ›´åŠ å®¹æ˜“åœ¨ sharp minimaå¤„æ”¶æ•›ï¼Œè¾ƒå°çš„batch sizeä½¿æ¨¡å‹åœ¨ flat minimaå¤„æ”¶æ•›ã€‚</p>

<p><strong>è®ºæ–‡äº®ç‚¹</strong></p>

<blockquote>
  <p>Many theoretical properties of these methods (SGD and its variants) are known. These include guarantees of:</p>

  <p>(a) convergence to minimizers of strongly-convex functions and to stationary points for non-convex functions,</p>

  <p>(b) saddle-point avoidance and</p>

  <p>(c) robustness to input data.</p>

  <p>Stochastic gradient methods have, however, a major drawback: owing to the sequential nature of the iteration and small batch sizes, there is limited avenue for parallelization.</p>
</blockquote>

<h3 id="å¤§-batch-size-çš„ç¼ºç‚¹">å¤§ batch size çš„ç¼ºç‚¹</h3>

<p>å¤§ batch size çš„æ³›åŒ–èƒ½åŠ›å·®æ˜¯å› ä¸ºï¼Œå®ƒå€¾å‘æ”¶æ•›åœ¨ sharp minima å¤„ã€‚ æå°å€¼çš„ sharpness æ˜¯ç”± \(\nabla ^2 f(x)\) æ­£ç‰¹å¾å€¼çš„å¤§å°æ¥å†³å®šçš„ï¼Œç‰¹å¾å€¼è¶Šå¤§ï¼Œæå°å€¼å°±è¶Š sharpï¼Œ æ³›åŒ–èƒ½åŠ›ä¹Ÿå°±è¶Šå·®ã€‚</p>

:ET