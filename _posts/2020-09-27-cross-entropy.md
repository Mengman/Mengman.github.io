---
layout: post
title: "机器学习中的交叉熵"
date: 2020-09-27 14:11:00 +0800
categories: machinelearning math
typora-root-url: ..
---

# 机器学习中的交叉熵



[toc]

交叉熵损失函数是机器学习分类问题中最常用的一个损失函数。但是很多人对于“交叉熵”的概念缺乏理解。本文的目的是希望能够通俗易懂的解释清楚 “熵”， “交叉熵” 和 “KL-散度” 这三个相关的概念。

## 信息论中的“熵”

“熵” (entropy) 这个概念在热力学中表示一个系统的无序程度，熵越大系统就越混乱。1948年香农在其发表的《通信的数学理论》论文中给出了”信息熵“的定义：信息熵是随机事件不确定性的度量。信息熵与热力学熵有着紧密的内在联系，不过在本文中所讨论的熵都是指信息熵。

### 有效信息长度

为了能更好的解释信息熵，我们先来看一个例子。有一个气象监测站，通过检测某地的气象数据，能给出第二天的天气预测。气象站需要将预测结果网络发送给气象中心。如果当地只有两种天气一种是晴天，一种是雨天，那么气象站如何能够高效的发送预测结果呢？最简单的情况下，气象站可以将预测结果以汉字的形式发送出去。如果预测是晴天就发送”晴天“，如果是雨天就发送”雨天“。我们知道一个汉字在计算机中通常的编码长度是 2 byte，也就是 8 bit，那么这两个汉字就是 16 bit。为了能更高效的发送信息，我们可以对信息进行编码，用 0 来表示晴天，1 来表示雨天。那么我们需要发送的信息长度就只有 1 bit 了。对于原来长度为 16 bit 的信息来说，它**真正有效的信息长度**其实只有 1 bit。

![weather_2](/assets/entropy1.png)



那如果当地天气有8种情况，我们需要多少长度的消息才能发送预测结果呢？答案很简单： $2^3=8$ ，我们只需要3个 bit 就能发送8种天气。在计算机中对于任意n种分类情况，我们只需要 $log_2 (n)$  长度的编码信息就可以把所有的情况进行编码。

![c](/assets/entropy2.png)



### 平均有效消息长度

我们知道在现实中，第二天出现什么天气的概率不是均等的，在夏天出现晴天和雨天的概率比较大，而出现雪天的概率很小；而在冬天出现雨天的概率较小，出现雪天的概率较大。在第一个例子中如果晴天的概率是 75%， 雨天的概率是 25%，那么消息的长度应该是多少呢？在计算消息长度之前，我们需要先介绍“熵减因子”(reduction factor)的概念。按照香农在论文中提出：

> 每发送1个bit的有效信息，能减少了接收者2个因子的不确定性。

**熵减因子** u 与有**效消息长度** m 之间的关系是： $2^u=m$

在已知事件概率的情况下，熵减因子 u 与概率 p 的关系是： $\frac{1}{p}=u$

基于熵减因子与概率的关系，我们可以根据概率 p 推断出有效信息的长度 m的关系 : $-log_2{p} = m$ 



![wheather_4](/assets/entropy4.png)

现在我们可以回答之前提出的问题了。

**晴天的有效信息长度** $ m_s = -log_2{p_s} = -log_2{0.75} = 0.41$ 

**雨天的有效信息长度** $m_r = -log_2{p_r} = -log_2{0.25} = 2$

**平均有效信息长度** $m_{avg} = p_s * (-log_2{p_s}) + p_r * (-log_2{p_r}) = 0.75 * 0.41 + 0.25 * 2 = 0.81$



平均有效信息长度又被称为**信息熵** , 数学定义为：
$$
\text{Entropy},H(p) = - \sum{p(i)*log(p(i))}
$$




我们按照现实情况，分别给第二个例子中8种天气加一个概率。

![weather_8_prob](/assets/entropy3.png)

按照信息熵的定义，我们可以计算出此时的信息熵
$$
\begin{aligned} 
\text{Entropy} = -0.35 * log_2{0.35}  \\
&& -0.35 * log_2{0.35} \\ 
&& -0.10 * log_2{0.10} \\
&& -0.10 * log_2{0.10} \\
&& -0.04 * log_2{0.04} \\ 
&& -0.04 * log_2{0.04} -0.01 * log_2{0.01} -0.01 * log_2{0.01} = 2.23
\end{aligned}
$$
